{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c591498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba00a90",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4076109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Saham Loaded: 43 rows\n",
      "Data Berita Loaded: 10 articles\n"
     ]
    }
   ],
   "source": [
    "# Load Price Data\n",
    "try:\n",
    "    df_price = pd.read_csv('data/ohlcv/BUMI_daily_oct_nov_2025_ohlcv.csv')\n",
    "    df_price.columns = df_price.columns.str.lower() \n",
    "    df_price['date'] = pd.to_datetime(df_price['date'])\n",
    "    df_price = df_price.sort_values('date').reset_index(drop=True)\n",
    "    print(f\"Data Saham Loaded: {len(df_price)} rows\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File 'BUMI_daily_ohlcv.csv' tidak ditemukan.\")\n",
    "    exit()\n",
    "\n",
    "# Load News Data\n",
    "try:\n",
    "    with open('data/news/filtered_BUMI_oct_nov_2025.json', 'r') as f:\n",
    "        news_data = json.load(f)\n",
    "    \n",
    "    df_news = pd.DataFrame(news_data)\n",
    "    df_news['date'] = pd.to_datetime(df_news['date'])\n",
    "    print(f\"Data Berita Loaded: {len(df_news)} articles\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File 'filtered_BUMI_oct_nov_2025.json' tidak ditemukan.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8eb024",
   "metadata": {},
   "source": [
    "## Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29242502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hitung Daily Return (%)\n",
    "df_price['daily_return'] = df_price['close'].pct_change() * 100\n",
    "\n",
    "# Logic Perhitungan Dampak (Event Study) \n",
    "def calculate_news_impact(news_df, price_df, look_ahead_days=3):\n",
    "    results = []\n",
    "    \n",
    "    # Baseline volatility (standar deviasi return harian seluruh periode)\n",
    "    # Digunakan untuk menilai apakah gerakan harga itu signifikan atau noise biasa\n",
    "    avg_volatility = price_df['daily_return'].std()\n",
    "    \n",
    "    for idx, row in news_df.iterrows():\n",
    "        news_date = row['date']\n",
    "        \n",
    "        # Sinkronisasi Tanggal: Cari hari bursa terdekat jika berita rilis weekend\n",
    "        market_date_rows = price_df[price_df['date'] >= news_date]\n",
    "        \n",
    "        if market_date_rows.empty:\n",
    "            continue\n",
    "            \n",
    "        market_date = market_date_rows['date'].min()\n",
    "        start_idx = price_df[price_df['date'] == market_date].index[0]\n",
    "        \n",
    "        # Tentukan window dampak (misal: Hari H s/d H+3)\n",
    "        end_idx = min(start_idx + look_ahead_days, len(price_df) - 1)\n",
    "        window_data = price_df.loc[start_idx:end_idx].copy()\n",
    "        \n",
    "        if window_data.empty:\n",
    "            continue\n",
    "\n",
    "        # --- METRIK 1: Cumulative Return (Total Kenaikan selama window) ---\n",
    "        # Kita fokus pada dampak POSITIF (Kenaikan Harga)\n",
    "        cum_return = window_data['daily_return'].sum()\n",
    "        \n",
    "        # Jika kumulatif return negatif, kita anggap probabilitas menaikkan harga = 0\n",
    "        impact_score = max(0, cum_return) \n",
    "        \n",
    "        # --- METRIK 2: Lag Detection (Kapan harga memuncak?) ---\n",
    "        # Mencari di hari keberapa (0, 1, 2, atau 3) return tertinggi terjadi\n",
    "        max_return_idx = window_data['daily_return'].idxmax()\n",
    "        days_lag = max_return_idx - start_idx\n",
    "        \n",
    "        results.append({\n",
    "            'date_news': news_date,\n",
    "            'date_market_reaction': market_date, # Tanggal pasar merespons\n",
    "            'title': row['title'],\n",
    "            'impact_return_pct': impact_score, # Seberapa besar harga naik\n",
    "            'lag_days': days_lag\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b19a86da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_impact = calculate_news_impact(df_news, df_price, look_ahead_days=3)\n",
    "\n",
    "# Normalisasi\n",
    "total_impact = df_impact['impact_return_pct'].sum()\n",
    "\n",
    "if total_impact > 0:\n",
    "    df_impact['probability_score'] = (df_impact['impact_return_pct'] / total_impact) * 100\n",
    "else:\n",
    "    df_impact['probability_score'] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc6c5c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HASIL ANALISIS PROBABILITAS DAMPAK BERITA (EVENT STUDY)\n",
      "================================================================================\n",
      " date_news                                                                         title  lag_days  impact_return_pct  probability_score\n",
      "2025-10-08                          BUMI Rampungkan Akuisisi Wolfram, Masuk Emasâ€“Tembaga         3               0.00                0.0\n",
      "2025-10-10   Momentum Bullish IHSG Diprediksi Terjaga: Perhatikan BUMI, DKFT, ERAL, WIFI         1               0.00                0.0\n",
      "2025-10-17                         Mulai Gersang, Saham BUMI hingga ENRG Ditinggal Asing         3               0.25                0.2\n",
      "2025-10-17                        IHSG Hari ini Diprediksi Koreksi: BoW PANI, BRIS, BUMI         3               0.25                0.2\n",
      "2025-11-02                               Margin BUMI Membaik, Laba Bersih Masih Tertekan         2               0.00                0.0\n",
      "2025-11-10         BUMI Kantongi Potensi Pendapatan Rp26 Triliun dari Wolfram, Kok Bisa?         1              52.02               42.1\n",
      "2025-11-11                               IHSG Ditutup Koreksi, BUMI Jadi 'Bintang Utama'         0              43.85               35.5\n",
      "2025-11-12 10 Saham Terlemah Hari ini, DAAZ dan BUMI Jadi Penekan di Tengah IHSG Menguat         1              12.76               10.3\n",
      "2025-11-14              Rasio Keuangan Ketat, BUMI Tetap Terbitkan Obligasi Rp780 Miliar         3               2.06                1.7\n",
      "2025-11-25         Asing Borong BUMI, Jual Besar-besaran BBRI: Ke Mana Arah Selanjutnya?         1              12.33               10.0\n",
      "\n",
      "[INFO] Hasil lengkap telah disimpan ke file: hasil_analisis_berita_bumi.csv\n"
     ]
    }
   ],
   "source": [
    "final_output = df_impact[['date_news', 'title', 'lag_days', 'impact_return_pct', 'probability_score']].sort_values('date_news')\n",
    "\n",
    "# Format angka agar mudah dibaca\n",
    "final_output['impact_return_pct'] = final_output['impact_return_pct'].round(2)\n",
    "final_output['probability_score'] = final_output['probability_score'].round(1)\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HASIL ANALISIS PROBABILITAS DAMPAK BERITA (EVENT STUDY)\")\n",
    "print(\"=\"*80)\n",
    "print(final_output.to_string(index=False)) # Print tabel rapi ke terminal\n",
    "\n",
    "\n",
    "output_filename = 'hasil_analisis_berita_bumi.csv'\n",
    "final_output.to_csv(output_filename, index=False)\n",
    "print(f\"\\n[INFO] Hasil lengkap telah disimpan ke file: {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87616f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import json\n",
    "# import numpy as np\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# import re\n",
    "\n",
    "# # 1. Load Data\n",
    "# news_file = 'data/news/filtered_BUMI_oct_nov_2025.json'\n",
    "# price_file = 'data/ohlcv/BUMI_daily_oct_nov_2025_ohlcv.csv'\n",
    "\n",
    "# with open(news_file, 'r', encoding='utf-8') as f:\n",
    "#     news_data = json.load(f)\n",
    "\n",
    "# news_df = pd.DataFrame(news_data)\n",
    "# news_df['date'] = pd.to_datetime(news_df['date'])\n",
    "\n",
    "# price_df = pd.read_csv(price_file)\n",
    "# price_df.columns = [col.replace('<', '').replace('>', '').lower() for col in price_df.columns]\n",
    "# price_df['date'] = pd.to_datetime(price_df['date'])\n",
    "# price_df = price_df.sort_values('date')\n",
    "\n",
    "# # 2. Financial Feature Engineering & Lag Effects\n",
    "# # Calculate daily returns\n",
    "# price_df['return'] = (price_df['close'] - price_df['open']) / price_df['open']\n",
    "# price_df['prev_close'] = price_df['close'].shift(1)\n",
    "# price_df['return_c2c'] = (price_df['close'] - price_df['prev_close']) / price_df['prev_close']\n",
    "\n",
    "# # Calculate future returns for lag effects (max return over next 3 days)\n",
    "# # Does the price go up significantly in the next 1-3 days?\n",
    "# price_df['future_close_1'] = price_df['close'].shift(-1)\n",
    "# price_df['future_close_2'] = price_df['close'].shift(-2)\n",
    "# price_df['future_close_3'] = price_df['close'].shift(-3)\n",
    "\n",
    "# price_df['max_future_close'] = price_df[['future_close_1', 'future_close_2', 'future_close_3']].max(axis=1)\n",
    "# price_df['max_future_return'] = (price_df['max_future_close'] - price_df['close']) / price_df['close']\n",
    "\n",
    "# # Target: 1 if the stock rises more than 1% in the next 3 days, else 0\n",
    "# price_df['target_up'] = (price_df['max_future_return'] > 0.01).astype(int)\n",
    "\n",
    "# # 3. Simple Indonesian Financial Sentiment Analyzer\n",
    "# positive_words = ['akuisisi', 'laba', 'naik', 'beli', 'untung', 'tinggi', 'borong', 'target', 'potensi', 'penguatan', 'bullish', 'positif', 'masuk', 'dorong', 'rampungkan']\n",
    "# negative_words = ['rugi', 'turun', 'jual', 'tekanan', 'lepas', 'gagal', 'anjlok', 'rendah', 'koreksi', 'bearish', 'negatif', 'keluar']\n",
    "\n",
    "# def get_sentiment(text):\n",
    "#     text = text.lower()\n",
    "#     pos_count = sum(len(re.findall(r'\\b' + word + r'\\b', text)) for word in positive_words)\n",
    "#     neg_count = sum(len(re.findall(r'\\b' + word + r'\\b', text)) for word in negative_words)\n",
    "#     total = pos_count + neg_count\n",
    "#     if total == 0: return 0\n",
    "#     return (pos_count - neg_count) / total\n",
    "\n",
    "# news_df['sentiment_score'] = news_df['full_content'].apply(get_sentiment)\n",
    "\n",
    "# # 4. Merge Data (Aligning News with Stock Market Days)\n",
    "# # If a news comes out on a weekend, it affects the next trading day.\n",
    "# # We will use pandas merge_asof for this, but standard merge with forward fill is easier.\n",
    "# merged_df = pd.merge(news_df, price_df, on='date', how='left')\n",
    "# # Fill missing price data for weekends with the next available trading day\n",
    "# merged_df = merged_df.sort_values('date')\n",
    "# merged_df['target_up'] = merged_df['target_up'].fillna(method='bfill')\n",
    "# merged_df['volume'] = merged_df['volume'].fillna(method='bfill')\n",
    "# merged_df['return'] = merged_df['return'].fillna(method='bfill')\n",
    "\n",
    "# # Drop rows where we can't find future price data\n",
    "# train_df = merged_df.dropna(subset=['target_up', 'volume'])\n",
    "\n",
    "# # 5. Machine Learning Model (Probability Scoring)\n",
    "# # Features: Sentiment Score, Current Volume (normalized)\n",
    "# scaler = MinMaxScaler()\n",
    "# train_df['vol_norm'] = scaler.fit_transform(train_df[['volume']])\n",
    "\n",
    "# X = train_df[['sentiment_score', 'vol_norm']]\n",
    "# y = train_df['target_up']\n",
    "\n",
    "# # Train Random Forest to get probabilities\n",
    "# clf = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=3)\n",
    "# clf.fit(X, y)\n",
    "\n",
    "# # Predict probabilities\n",
    "# train_df['prob_up'] = clf.predict_proba(X)[:, 1] # Probability of Class 1 (Up)\n",
    "\n",
    "# # Clean up output for the user\n",
    "# output_df = train_df[['date', 'title', 'sentiment_score', 'max_future_return', 'prob_up']].copy()\n",
    "# output_df['prob_up_pct'] = (output_df['prob_up'] * 100).round(2)\n",
    "# output_df['sentiment_score'] = output_df['sentiment_score'].round(2)\n",
    "# output_df['max_future_return'] = (output_df['max_future_return'] * 100).round(2)\n",
    "\n",
    "# print(output_df[['date', 'title', 'prob_up_pct']].to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
